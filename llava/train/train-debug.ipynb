{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "import tokenizers\n",
    "\n",
    "from llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from torch.utils.data import Dataset\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "from llava.model import *\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.eval.run_llava import eval_model\n",
    "\n",
    "# model_path = \"/datadrive_a/jihai/LLaVA/scripts/v1_5/checkpoints/llava-v1.5-7b-lora\"\n",
    "\n",
    "# tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "#     model_path=model_path,\n",
    "#     model_base='/datadrive_a/jihai/azure_storage2/vigstandard_data/jihai/checkpoint/vicuna-7b-v1.5/vicuna-7b-v1.5',\n",
    "#     model_name=get_model_name_from_path(model_path),\n",
    "#     # device=\"cuda:2\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datadrive_a/tmp/vicuna-7b-v1.5/vicuna-7b-v1.5\n",
      "linear\n"
     ]
    }
   ],
   "source": [
    "class ModelArguments:\n",
    "    model_name_or_path = \"/datadrive_a/tmp/vicuna-7b-v1.5/vicuna-7b-v1.5\"\n",
    "    version = \"v0\"\n",
    "    freeze_backbone = True\n",
    "    tune_mm_mlp_adapter = False\n",
    "    vision_tower = 'openai/clip-vit-large-patch14'\n",
    "    vision_tower_path = '/datadrive_a/jihai/tmp'\n",
    "    mm_vision_select_layer = -2  # default to the last layer\n",
    "    pretrain_mm_mlp_adapter = None\n",
    "    mm_projector_type = \"linear\"\n",
    "    mm_use_im_start_end = False\n",
    "    mm_use_im_patch_token = True\n",
    "    mm_patch_merge_type = \"flat\"\n",
    "    mm_vision_select_feature = \"patch\"\n",
    "    vision_tower_gen = 'same'\n",
    "    vision_tower_gen_path = None\n",
    "    image_loss='mse'\n",
    "    pretrained_mm_mlp_adapter=None\n",
    "    mm_projector_head_output_size=None\n",
    "\n",
    "# Usage example in Jupyter:\n",
    "args = ModelArguments()\n",
    "print(args.model_name_or_path)\n",
    "print(args.mm_projector_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/datadrive_a/jihai/miniconda3/envs/llava/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.82s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "model = LlavaLlamaForCausalLM_ImgGen.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai/clip-vit-large-patch14 is already loaded, `load_model` called again, skipping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.get_model().initialize_vision_modules(\n",
    "    model_args=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataArguments(data_path='/datadrive_a/jihai/data/multimodalout/smart_watch_train.json', lazy_preprocess=True, is_multimodal=True, multimodal_out=True, image_folder='/datadrive_a/jihai/data/multimodalout/smart_watch_image_train', image_aspect_ratio='square', understanding_only=False, generation_only=True, image_shape=[3, 224, 224], num_image_token=256, dataset='smartwatch')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default='/datadrive_a/jihai/data/multimodalout/smart_watch_train.json',\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = True\n",
    "    is_multimodal: bool = True\n",
    "    multimodal_out: bool=True\n",
    "    image_folder: Optional[str] = field(default='/datadrive_a/jihai/data/multimodalout/smart_watch_image_train')\n",
    "    image_aspect_ratio: str = 'square'\n",
    "    understanding_only: bool=False\n",
    "    generation_only: bool=True\n",
    "    image_shape: List[int] = field(\n",
    "        default_factory=lambda: [3,224,224],\n",
    "    )\n",
    "    num_image_token: int = 256 #how many token will one image take up\n",
    "    dataset: str = 'smartwatch'\n",
    "\n",
    "data_args = DataArguments()\n",
    "data_args.image_processor=nn.Identity()\n",
    "data_args.image_processor_gen=nn.Identity()\n",
    "print(data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_v1_with_gen(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    has_image: bool = False,\n",
    "    num_image_token: int = 6 #how many token will one image take up\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess conversations for generation.\n",
    "    original preprocess_v1() will return:\n",
    "    {'input_ids': tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
    "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
    "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
    "         29889,  3148,  1001, 29901, 29871,  -200, 29871,    13,  5618,   931,\n",
    "           338,   372,   297,   278,  1967, 29973,   319,  1799,  9047, 13566,\n",
    "         29901,   739,   338, 29871, 29896, 29896, 29901, 29906, 29945, 29901,\n",
    "         29941, 29900,     2]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,   739,   338, 29871, 29896, 29896, 29901, 29906, 29945, 29901,\n",
    "         29941, 29900,     2]])}\n",
    "    when '-200' is in the target (answer) for generation, need to:\n",
    "     1. replace it with the id of '<image>' as the indicator for generation.\n",
    "     2. add num_of_image_tokens of '-100' after the id of '<image>' as image token holders\n",
    "     3. add img_begin index to data_dict such that hidden_state[:,img_begin:img_begin+num_of_image_tokens] will be the image tokens\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    conv = conversation_lib.default_conversation.copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "\n",
    "    if has_image:\n",
    "        print('has image')\n",
    "        input_ids = torch.stack([tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\n",
    "        print(input_ids)\n",
    "    else:\n",
    "        input_ids = tokenizer(\n",
    "            conversations,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ).input_ids\n",
    "\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == conversation_lib.SeparatorStyle.TWO\n",
    "\n",
    "    # Mask targets\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_INDEX\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "\n",
    "            if has_image:\n",
    "                round_len = len(tokenizer_image_token(rou, tokenizer))\n",
    "                instruction_len = len(tokenizer_image_token(parts[0], tokenizer)) - 2\n",
    "            else:\n",
    "                round_len = len(tokenizer(rou).input_ids)\n",
    "                instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\n",
    "                round_len -= 1\n",
    "                instruction_len -= 1\n",
    "\n",
    "            target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                target[:] = IGNORE_INDEX\n",
    "                print(\n",
    "                    f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                    f\" (ignored)\"\n",
    "                )\n",
    "    #-----------add preprocess for generation:\n",
    "    img_token_start=None #mark whether there is image to generate, and the generated image position\n",
    "    if DEFAULT_IMAGE_TOKEN in sources[0][1]['value']:\n",
    "        #sp token '<image>'\n",
    "        gen_indicator=torch.LongTensor([[529,  3027, 29958]])\n",
    "        # 查找 -200 的位置\n",
    "        positions = (input_ids == -200).nonzero(as_tuple=True)\n",
    "        # 对 input_ids 进行处理\n",
    "        for pos in zip(*positions):    \n",
    "            # 在其后面插入 num_image_token 个 -100\n",
    "            insert_position = pos[1]  # delete -200, insert gen_indicator and num_image_token -300. -300 need to be replaced by actual image token.\n",
    "            img_token_start=insert_position+gen_indicator.size(1)\n",
    "            input_ids = torch.cat((input_ids[:, :insert_position], \n",
    "                                    gen_indicator,\n",
    "                                    torch.full((input_ids.size(0), num_image_token), -300, dtype=torch.long), \n",
    "                                    input_ids[:, insert_position+1:]), dim=1)\n",
    "\n",
    "        # 对 labels 进行处理（相同的处理方式）\n",
    "        for pos in zip(*positions):\n",
    "            insert_position = pos[1]  # delete -200, insert gen_indicator and num_image_token -100\n",
    "            targets = torch.cat((targets[:, :insert_position], \n",
    "                                gen_indicator,\n",
    "                                torch.full((targets.size(0), num_image_token), -100, dtype=torch.long), \n",
    "                                targets[:, insert_position+1:]), dim=1)\n",
    "\n",
    "    return dict(\n",
    "        img_token_start=img_token_start,\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "    )\n",
    "\n",
    "class LazySupervisedDataset_SmartWatch(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args: DataArguments):\n",
    "        super(LazySupervisedDataset_SmartWatch, self).__init__()\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "        if data_args.generation_only:\n",
    "            list_data_dict = [e for e in list_data_dict if e['task']==\"generation\"]\n",
    "        if data_args.understanding_only:\n",
    "            list_data_dict = [e for e in list_data_dict if (e['task']==\"vqa\" or e['task']==\"caption\")]\n",
    "\n",
    "        #rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.list_data_dict = list_data_dict\n",
    "        self.data_args = data_args\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    @property\n",
    "    def modality_lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            cur_len = sum(len(conv['value'].split()) for conv in sample['conversations'])\n",
    "            cur_len = cur_len if 'image' in sample else -cur_len\n",
    "            length_list.append(cur_len)\n",
    "        return length_list\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "        if 'image' in sources[0]:\n",
    "            image_file = self.list_data_dict[i]['image']\n",
    "            image_folder = self.data_args.image_folder\n",
    "            if sources[0]['task']=='generation':\n",
    "                processor = self.data_args.image_processor_gen\n",
    "            else:\n",
    "                processor=self.data_args.image_processor\n",
    "            image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\n",
    "            if self.data_args.image_aspect_ratio == 'pad':\n",
    "                def expand2square(pil_img, background_color):\n",
    "                    width, height = pil_img.size\n",
    "                    if width == height:\n",
    "                        return pil_img\n",
    "                    elif width > height:\n",
    "                        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                        result.paste(pil_img, (0, (width - height) // 2))\n",
    "                        return result\n",
    "                    else:\n",
    "                        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                        return result\n",
    "                image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n",
    "            transform = transforms.ToTensor()\n",
    "            image = transform(image)\n",
    "      \n",
    "            #     image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "            # else:\n",
    "            #     image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "\n",
    "\n",
    "            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\n",
    "        else:\n",
    "            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\n",
    "        #bypass preprocess(), use default for llava1.5: preprocess_v1\n",
    "        #print(('image' in self.list_data_dict[i]))\n",
    "        data_dict = preprocess_v1_with_gen(\n",
    "            sources,\n",
    "            self.tokenizer,\n",
    "            has_image=('image' in self.list_data_dict[i]),\n",
    "            num_image_token=self.data_args.num_image_token\n",
    "        )\n",
    "        \n",
    "        if isinstance(i, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                             labels=data_dict[\"labels\"][0],\n",
    "                             img_token_start=data_dict[\"img_token_start\"])\n",
    "            #print(f\"data_dict[img_token_start]:{data_dict['img_token_start']}\")\n",
    "\n",
    "        # image exist in the data\n",
    "        if 'image' in self.list_data_dict[i]:\n",
    "            data_dict['image'] = image\n",
    "            #print(image.shape)\n",
    "        elif self.data_args.is_multimodal:\n",
    "            # image does not exist in the data, but the model is multimodal\n",
    "            # this will skip in my_prepare_inputs_labels_for_multimodal, not input to the model\n",
    "            data_dict['image'] = torch.zeros(self.data_args.image_shape)\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "    data_args: DataArguments\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels, img_token_start = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\", \"img_token_start\"))\n",
    "\n",
    "        #generation padding sample\n",
    "        input_ids_pad=torch.ones((5+self.data_args.num_image_token,), dtype=torch.long)\n",
    "        labels_pad=torch.ones((5+self.data_args.num_image_token,), dtype=torch.long)\n",
    "        input_ids_pad[5:]=-300\n",
    "        labels_pad[5:]=-100\n",
    "        input_ids.append(input_ids_pad)\n",
    "        labels.append(labels_pad)\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        \n",
    "        img_token_start_pad=torch.ones((1,), dtype=torch.long)*5\n",
    "        # input_ids = torch.cat((input_ids, input_ids_pad), dim=0)\n",
    "        # labels = torch.cat((labels, labels_pad), dim=0)\n",
    "        img_token_start.append(img_token_start_pad)\n",
    "        image_pad=torch.zeros(self.data_args.image_shape, dtype=torch.float32,device=input_ids.device)\n",
    "\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "            img_token_start=img_token_start\n",
    "        )\n",
    "\n",
    "        if 'image' in instances[0]:\n",
    "            images = [instance['image'] for instance in instances]\n",
    "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                batch['images'] = torch.stack(images)\n",
    "            else:\n",
    "                batch['images'] = images\n",
    "            batch['images']= torch.cat((batch['images'], image_pad.unsqueeze(0)), dim=0)\n",
    "        #print(batch['images'].shape)\n",
    "        return batch\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    if data_args.dataset=='smartwatch':\n",
    "        train_dataset = LazySupervisedDataset_SmartWatch(tokenizer=tokenizer,\n",
    "                                    data_path=data_args.data_path,\n",
    "                                    data_args=data_args)\n",
    "    # elif data_args.dataset=='segment_digit':\n",
    "    #     train_dataset = LazySupervisedDataset_ImgGen(tokenizer=tokenizer,\n",
    "    #                                 data_path=data_args.data_path,\n",
    "    #                                 data_args=data_args)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer,data_args=data_args)\n",
    "    return dict(train_dataset=train_dataset,\n",
    "                eval_dataset=None,\n",
    "                data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has image\n",
      "tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901,  1815,   366,  5706,   263,  1967,   310,\n",
      "           263, 15040,  6505, 29892,   373,   607,   278,  1857,   931,   338,\n",
      "         29871, 29900, 29953, 29901, 29900, 29953, 29901, 29945, 29906, 29892,\n",
      "           411, 11015,  1361,   297, 18350, 29899,  1182,   523, 29899,  1182,\n",
      "           523, 29892, 23425, 11785, 29899,  4366, 29899,  1182,   523,  1473,\n",
      "          1361, 29892,   411,  6501, 29899,  1182,   523, 29899, 26031,  8296,\n",
      "          1426,  2955,  3239, 29892, 16384, 29871, 29945, 29941, 29995,  3081,\n",
      "          3233, 29889,   319,  1799,  9047, 13566, 29901,   450,  1967,   338,\n",
      "          5759, 29889, 29871,    13,  -200,     2]])\n",
      "{'input_ids': tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901,  1815,   366,  5706,   263,  1967,   310,\n",
      "           263, 15040,  6505, 29892,   373,   607,   278,  1857,   931,   338,\n",
      "         29871, 29900, 29953, 29901, 29900, 29953, 29901, 29945, 29906, 29892,\n",
      "           411, 11015,  1361,   297, 18350, 29899,  1182,   523, 29899,  1182,\n",
      "           523, 29892, 23425, 11785, 29899,  4366, 29899,  1182,   523,  1473,\n",
      "          1361, 29892,   411,  6501, 29899,  1182,   523, 29899, 26031,  8296,\n",
      "          1426,  2955,  3239, 29892, 16384, 29871, 29945, 29941, 29995,  3081,\n",
      "          3233, 29889,   319,  1799,  9047, 13566, 29901,   450,  1967,   338,\n",
      "          5759, 29889, 29871,    13,   529,  3027, 29958,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,     2],\n",
      "        [    1,     1,     1,     1,     1,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   450,  1967,   338,\n",
      "          5759, 29889, 29871,    13,   529,  3027, 29958,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,     2],\n",
      "        [    1,     1,     1,     1,     1,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100]]), 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False]]), 'img_token_start': [tensor(117), tensor([5])], 'images': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 创建数据模块\n",
    "data_module = make_supervised_data_module(tokenizer, data_args)\n",
    "\n",
    "# 提取 train_dataset 和 data_collator\n",
    "train_dataset = data_module['train_dataset']\n",
    "data_collator = data_module['data_collator']\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=data_collator)\n",
    "\n",
    "# 获取一个batch的数据\n",
    "data_iter=iter(train_dataloader)\n",
    "batch = next(data_iter)\n",
    "\n",
    "# 打印 batch 内容以进行调试\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901,  1815,   366,  5706,   263,  1967,   310,\n",
      "           263, 15040,  6505, 29892,   373,   607,   278,  1857,   931,   338,\n",
      "         29871, 29900, 29953, 29901, 29900, 29953, 29901, 29945, 29906, 29892,\n",
      "           411, 11015,  1361,   297, 18350, 29899,  1182,   523, 29899,  1182,\n",
      "           523, 29892, 23425, 11785, 29899,  4366, 29899,  1182,   523,  1473,\n",
      "          1361, 29892,   411,  6501, 29899,  1182,   523, 29899, 26031,  8296,\n",
      "          1426,  2955,  3239, 29892, 16384, 29871, 29945, 29941, 29995,  3081,\n",
      "          3233, 29889,   319,  1799,  9047, 13566, 29901,   450,  1967,   338,\n",
      "          5759, 29889, 29871,    13,   529,  3027, 29958,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,     2],\n",
      "        [    1,     1,     1,     1,     1,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,  -300,\n",
      "          -300,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   450,  1967,   338,\n",
      "          5759, 29889, 29871,    13,   529,  3027, 29958,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,     2],\n",
      "        [    1,     1,     1,     1,     1,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100]]), 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False]]), 'img_token_start': [tensor(117), tensor([5])], 'images': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])}\n",
      "[tensor(117), tensor([5])]\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "#batch = next(data_iter)\n",
    "print(batch)\n",
    "for s in batch['img_token_start']:\n",
    "    if s is not None:\n",
    "        s.cuda()\n",
    "print(batch['img_token_start'])\n",
    "print(batch['images'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['images']=torch.zeros(batch['images'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8065, -0.5087],\n",
      "        [-0.1101,  0.3153]])\n"
     ]
    }
   ],
   "source": [
    "print(model.get_model().vision_tower.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_model().cuda()\n",
    "(\n",
    "    input_ids,\n",
    "    position_ids,\n",
    "    attention_mask,\n",
    "    past_key_values,\n",
    "    inputs_embeds,\n",
    "    labels\n",
    ") = model.my_prepare_inputs_labels_for_multimodal(\n",
    "    input_ids=batch['input_ids'].cuda(),\n",
    "    position_ids=None,\n",
    "    attention_mask=batch['attention_mask'].cuda(),\n",
    "    past_key_values=None,\n",
    "    labels=batch['labels'].cuda(),\n",
    "    images=batch['images'].cuda(),\n",
    "    img_start_token=batch['img_token_start'],\n",
    "    image_sizes=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaLlamaForCausalLM_ImgGen(\n",
       "  (model): LlavaLlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "    (vision_tower): SyntheticVisionTower(\n",
       "      (image_processor): Identity()\n",
       "    )\n",
       "    (mm_projector): Linear(in_features=7, out_features=4096, bias=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  (im_head): Linear(in_features=4096, out_features=7, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=model.float()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "output_attentions=None\n",
    "output_hidden_states=None\n",
    "output_attentions = output_attentions if output_attentions is not None else model.config.output_attentions\n",
    "output_hidden_states = (\n",
    "    output_hidden_states if output_hidden_states is not None else model.config.output_hidden_states\n",
    ")\n",
    "return_dict=None\n",
    "use_cache=None\n",
    "return_dict = return_dict if return_dict is not None else model.config.use_return_dict\n",
    "\n",
    "# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "outputs = model.model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    position_ids=position_ids,\n",
    "    past_key_values=past_key_values,\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    use_cache=use_cache,\n",
    "    output_attentions=output_attentions,\n",
    "    output_hidden_states=output_hidden_states,\n",
    "    return_dict=return_dict,\n",
    ")\n",
    "\n",
    "hidden_states = outputs[0]\n",
    "if model.config.pretraining_tp > 1:\n",
    "    lm_head_slices = model.lm_head.weight.split(model.vocab_size // model.config.pretraining_tp, dim=0)\n",
    "    logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(model.config.pretraining_tp)]\n",
    "    logits = torch.cat(logits, dim=-1)\n",
    "else:\n",
    "    logits = model.lm_head(hidden_states)\n",
    "\n",
    "logits = logits.float()\n",
    "\n",
    "loss = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_token_start=batch['img_token_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_positions: tensor([82, 81, -1], device='cuda:0')\n",
      "img_col_indices: tensor([[82, 83, 84, 85, 86, 87],\n",
      "        [81, 82, 83, 84, 85, 86]], device='cuda:0')\n",
      "img_row_indices: tensor([[0],\n",
      "        [1]], device='cuda:0')\n",
      "torch.Size([3, 6, 7])\n",
      "img_targets: tensor([[[0.6684, 0.6684, 0.0000, 0.6684, 0.6684, 0.0000, 0.6684],\n",
      "         [0.6684, 0.6684, 0.6684, 0.6684, 0.0000, 0.0000, 0.6684],\n",
      "         [0.6684, 0.6684, 0.0000, 0.6684, 0.6684, 0.0000, 0.6684],\n",
      "         [0.6684, 0.6684, 0.6684, 0.6684, 0.0000, 0.6684, 0.6684],\n",
      "         [0.0000, 0.6684, 0.6684, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.6684, 0.6684, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.6114, 0.6114, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6114, 0.6114, 0.6114, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.6114, 0.6114, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.6114, 0.6114, 0.6114, 0.6114, 0.0000, 0.0000, 0.6114],\n",
      "         [0.6114, 0.0000, 0.6114, 0.6114, 0.0000, 0.6114, 0.6114],\n",
      "         [0.6114, 0.6114, 0.6114, 0.0000, 0.0000, 0.0000, 0.0000]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 6, 7])\n",
      "torch.Size([2, 6, 4096])\n",
      "img_embed_outputs: tensor([[[-2.0684,  0.6624,  1.6202,  ..., -0.5578,  0.0721,  0.7821],\n",
      "         [-2.3157,  0.0272, -2.2888,  ..., -1.7146, -2.5507,  1.1488],\n",
      "         [-1.4304, -0.5334, -0.9632,  ..., -2.3023, -3.2352,  0.6651],\n",
      "         [-1.6928, -1.0104, -2.0722,  ..., -3.8772, -3.0828,  1.4324],\n",
      "         [-2.1612, -0.8557, -1.2932,  ..., -3.9315, -3.3347,  1.6286],\n",
      "         [-1.4424, -0.7168, -3.1492,  ..., -3.6590, -5.9317,  0.5976]],\n",
      "\n",
      "        [[-2.7326,  1.2312,  1.2481,  ..., -0.7809,  0.6416,  1.3056],\n",
      "         [-0.5504, -2.4341, -2.0553,  ..., -1.8949, -4.0706,  0.7769],\n",
      "         [-1.5128, -0.7359, -0.6290,  ..., -2.2739, -4.6086,  0.4454],\n",
      "         [-1.8062, -1.3043, -2.8470,  ..., -2.8403, -4.9170,  1.2442],\n",
      "         [-1.5185, -0.6815, -0.2316,  ..., -3.9859, -3.1407,  1.8312],\n",
      "         [-2.3302,  0.4700, -0.8087,  ..., -2.7694, -2.2962,  0.8759]]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "loss_img: 5.957004547119141\n",
      "loss_language: 2.3659613132476807\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "if labels is not None:\n",
    "    # 将 img_token_start 转为张量，如果是 None 则用 -1 占位\n",
    "    start_positions = torch.tensor([pos if pos is not None else -1 for pos in img_token_start], dtype=torch.long, device=inputs_embeds.device)\n",
    "\n",
    "    print(f\"start_positions: {start_positions}\")\n",
    "    # torch.save(start_positions, \"/datadrive_a/jihai/tmp/start_positions.pt\")\n",
    "    \n",
    "    # 创建一个掩码，标记有效的起始位置\n",
    "    valid_mask = start_positions >= 0\n",
    "\n",
    "    # 筛选出有效的 batch 索引和对应的开始位置\n",
    "    batch_indices = valid_mask.nonzero(as_tuple=True)[0]\n",
    "    start_indices = start_positions[valid_mask]\n",
    "\n",
    "    # 使用高级索引从 inputs_embeds 中抽取 img_token_start 到 img_token_start + seq_len 的部分\n",
    "    img_row_indices = batch_indices.unsqueeze(1)\n",
    "    img_col_indices = start_indices.unsqueeze(1) + torch.arange(6,device=start_indices.device).unsqueeze(0)\n",
    "    print(f\"img_col_indices: {img_col_indices}\")\n",
    "    print(f\"img_row_indices: {img_row_indices}\")\n",
    "    #img_embed_targets = inputs_embeds[img_row_indices, img_col_indices].detach()\n",
    "    images=batch['images'].cuda()\n",
    "    print(images.shape)\n",
    "    img_targets=images[img_row_indices].squeeze()\n",
    "    print(f\"img_targets: {img_targets}\")\n",
    "    print(img_targets.shape)\n",
    "    \n",
    "    #-------only for debug\n",
    "    hidden_states=hidden_states.cuda(0)\n",
    "    # print(hidden_states)\n",
    "    # print(f\"hidden_states_shape: {hidden_states.shape}\")\n",
    "    # print(f\"labels_shape:{labels.shape}\")\n",
    "    # print(f\"inputs_embeds_shape:{inputs_embeds.shape}\")\n",
    "\n",
    "    #get img embedding\n",
    "    img_col_indices-=1\n",
    "    img_embed_outputs = hidden_states[img_row_indices, img_col_indices]\n",
    "    print(img_embed_outputs.shape)\n",
    "\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    # Flatten the tokens\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "    shift_logits = shift_logits.view(-1, model.config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(shift_logits.device)\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "    loss_gen_fn = MSELoss()\n",
    "    loss_gen=loss_gen_fn(model.im_head(img_embed_outputs), img_targets)\n",
    "    print(f\"img_embed_outputs: {img_embed_outputs}\")\n",
    "    print(f\"loss_img: {loss_gen}\")\n",
    "    if torch.isnan(loss_gen): \n",
    "        loss_gen = torch.tensor(0.0, device=loss.device)\n",
    "    \n",
    "    print(f\"loss_language: {loss}\")\n",
    "\n",
    "# if not return_dict:\n",
    "#     output = (logits,) + outputs[1:]\n",
    "#     return (loss,) + output if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "None\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.int64\n",
      "torch.Size([1, 81])\n",
      "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "           450,  1967,  3697,   263,  2479,  5183, 29871, 29896, 29941, 29901,\n",
      "         29900, 29946, 29901, 29896, 29900,   297,  3708,   552,  2927, 29889,\n",
      "             2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(os.environ.get(\"CUDA_LAUNCH_BLOCKING\"))  # 输出 \"1\" 表示已成功设置\n",
    "print(input_ids)\n",
    "print(inputs_embeds.dtype)\n",
    "print(batch['images'].dtype)\n",
    "print(batch['labels'].dtype)\n",
    "print(labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cuda:3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_token_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimg_token_start\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/datadrive_a/jihai/miniconda3/envs/llava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/datadrive_a/jihai/miniconda3/envs/llava/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/datadrive_a/jihai/LLaVA/llava/model/language_model/llava_llama.py:160\u001b[0m, in \u001b[0;36mLlavaLlamaForCausalLM_ImgGen.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, image_sizes, return_dict, cache_position, num_logits_to_keep, img_token_start, **loss_kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m#get img embedding\u001b[39;00m\n\u001b[1;32m    159\u001b[0m img_col_indices\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 160\u001b[0m img_embed_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_row_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_col_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n\u001b[1;32m    163\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cuda:3)"
     ]
    }
   ],
   "source": [
    "print(model.device)\n",
    "model=model.float()\n",
    "\n",
    "model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda(), labels=batch['labels'].cuda(), img_token_start=batch['img_token_start'],images=batch['images'].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "         29889,  3148,  1001, 29901,  3251,   403,   385,  1967,  6445, 29871,\n",
      "         29900, 29896, 29901, 29945, 29896, 29901, 29906, 29941,   773,  3708,\n",
      "           552, 13340, 29889,   319,  1799,  9047, 13566, 29901,   530,  1967,\n",
      "           310, 29871, 29900, 29896, 29901, 29945, 29896, 29901, 29906, 29941,\n",
      "           411,  3694,   297,  3708,   552,   338,  4318, 29889, 29871,    13,\n",
      "           529,  3027, 29958,  -300,  -300,  -300,  -300,  -300,  -300,     2]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   530,  1967,\n",
      "           310, 29871, 29900, 29896, 29901, 29945, 29896, 29901, 29906, 29941,\n",
      "           411,  3694,   297,  3708,   552,   338,  4318, 29889, 29871,    13,\n",
      "           529,  3027, 29958,  -100,  -100,  -100,  -100,  -100,  -100,     2]]), 'attention_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True]]), 'img_token_start': [tensor(83)], 'images': tensor([[[0.8178, 0.8178, 0.8178, 0.8178, 0.8178, 0.8178, 0.0000],\n",
      "         [0.0000, 0.8178, 0.8178, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8178, 0.0000, 0.8178, 0.8178, 0.0000, 0.8178, 0.8178],\n",
      "         [0.0000, 0.8178, 0.8178, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.8178, 0.8178, 0.0000, 0.8178, 0.8178, 0.0000, 0.8178],\n",
      "         [0.8178, 0.8178, 0.8178, 0.8178, 0.0000, 0.0000, 0.8178]]])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(data_iter)\n",
    "\n",
    "# 打印 batch 内容以进行调试\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(batch['images'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   529,  3027, 29958]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "conversations=['<image>']\n",
    "print(tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        ))\n",
    "# reversed=tokenizer.batch_decode(data_dict['input_ids'][:,:35], skip_special_tokens=True)\n",
    "# print(reversed)\n",
    "# reversed=tokenizer.batch_decode(data_dict['input_ids'][:,36:], skip_special_tokens=True)\n",
    "# print(reversed)\n",
    "# reversed=tokenizer.batch_decode(data_dict['labels'][:,-12:], skip_special_tokens=True)\n",
    "# print(reversed)\n",
    "reversed=tokenizer.batch_decode(torch.LongTensor([[   529,  3027, 29958]]), skip_special_tokens=False)\n",
    "print(reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True]])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68])\n"
     ]
    }
   ],
   "source": [
    "image_features=torch.zeros(1,6,4096)+1\n",
    "input_ids=torch.LongTensor([[    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
    "         21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
    "           322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
    "         29889,  3148,  1001, 29901,  3529,  1510,   278,  1967,   310, 29871,\n",
    "         29896, 29896, 29901, 29906, 29945, 29901, 29941, 29900, 29889,   319,\n",
    "          1799,  9047, 13566, 29901,  2266,   338,   278,  1967, 29889,   529,\n",
    "          3027, 29958,  -300,  -300,  -300,  -300,  -300,  -300,     2]])\n",
    "labels=torch.LongTensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  2266,   338,   278,  1967, 29889,   529,\n",
    "          3027, 29958,  -100,  -100,  -100,  -100,  -100,  -100,     2]])\n",
    "attention_mask=input_ids.ne(tokenizer.pad_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(attention_mask)\n",
    "position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "print(position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "        29889,  3148,  1001, 29901,  3529,  1510,   278,  1967,   310, 29871,\n",
      "        29896, 29896, 29901, 29906, 29945, 29901, 29941, 29900, 29889,   319,\n",
      "         1799,  9047, 13566, 29901,  2266,   338,   278,  1967, 29889,   529,\n",
      "         3027, 29958,  -300,  -300,  -300,  -300,  -300,  -300,     2])]\n",
      "[tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  2266,   338,   278,  1967, 29889,   529,\n",
      "         3027, 29958,  -100,  -100,  -100,  -100,  -100,  -100,     2])]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "print(input_ids)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "        29889,  3148,  1001, 29901,  3529,  1510,   278,  1967,   310, 29871,\n",
      "        29896, 29896, 29901, 29906, 29945, 29901, 29941, 29900, 29889,   319,\n",
      "         1799,  9047, 13566, 29901,  2266,   338,   278,  1967, 29889,   529,\n",
      "         3027, 29958,  -300,  -300,  -300,  -300,  -300,  -300,     2])\n",
      "tensor(0)\n",
      "tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
      "        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
      "          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
      "        29889,  3148,  1001, 29901,  3529,  1510,   278,  1967,   310, 29871,\n",
      "        29896, 29896, 29901, 29906, 29945, 29901, 29941, 29900, 29889,   319,\n",
      "         1799,  9047, 13566, 29901,  2266,   338,   278,  1967, 29889,   529,\n",
      "         3027, 29958,     0,     0,     0,     0,     0,     0,     2])\n",
      "tensor([-0.7243, -0.8203,  0.2281,  0.6975, -1.8897, -0.4315, -0.3468, -0.0817,\n",
      "         0.4054, -1.0998,  0.3970, -0.4226, -0.4279,  2.0628,  0.2060, -0.1329,\n",
      "         0.8070,  0.7765,  1.6546,  0.2474,  0.0134,  0.6607,  0.0601,  0.0922,\n",
      "        -1.2661, -1.1331,  0.4618, -1.5803,  0.0869, -0.4767, -1.6443,  0.4182,\n",
      "         1.9648, -1.2589, -0.5343, -0.8887, -0.7140, -0.2978,  0.5701, -0.4634,\n",
      "        -0.7668, -0.0274,  0.1337, -0.8048, -0.2599, -1.5478,  0.8192,  1.3321,\n",
      "         0.0529,  0.5125,  0.6994,  0.7194,  0.6131, -0.1397, -1.5955, -1.0884,\n",
      "        -0.1014,  0.2556,  0.6350,  0.7482, -0.0822,  0.1914, -1.4717,  0.0493,\n",
      "        -0.5305,  0.4749, -0.8588, -1.1445,  1.7559])\n",
      "tensor([-0.7243, -0.8203,  0.2281,  0.6975, -1.8897, -0.4315, -0.3468, -0.0817,\n",
      "         0.4054, -1.0998,  0.3970, -0.4226, -0.4279,  2.0628,  0.2060, -0.1329,\n",
      "         0.8070,  0.7765,  1.6546,  0.2474,  0.0134,  0.6607,  0.0601,  0.0922,\n",
      "        -1.2661, -1.1331,  0.4618, -1.5803,  0.0869, -0.4767, -1.6443,  0.4182,\n",
      "         1.9648, -1.2589, -0.5343, -0.8887, -0.7140, -0.2978,  0.5701, -0.4634,\n",
      "        -0.7668, -0.0274,  0.1337, -0.8048, -0.2599, -1.5478,  0.8192,  1.3321,\n",
      "         0.0529,  0.5125,  0.6994,  0.7194,  0.6131, -0.1397, -1.5955, -1.0884,\n",
      "        -0.1014,  0.2556,  0.6350,  0.7482, -0.0822,  0.1914,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.7559])\n"
     ]
    }
   ],
   "source": [
    "new_input_embeds = []\n",
    "new_labels = []\n",
    "cur_image_idx = 0\n",
    "img_start_token=torch.LongTensor([62])\n",
    "for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "    print(cur_input_ids)\n",
    "    num_images = (cur_input_ids == IMAGE_TOKEN_INDEX).sum()\n",
    "    print(num_images)\n",
    "    if num_images == 0:\n",
    "        cur_image_features = image_features[cur_image_idx]\n",
    "        if img_start_token[batch_idx] is not None: #for generation process\n",
    "            cur_input_ids[cur_input_ids == -300] = 0 #replace the image token with 0 so that embedding can process\n",
    "            print(cur_input_ids)\n",
    "            cur_input_embeds = torch.randn(cur_input_ids.size(0), 4096)\n",
    "            print(cur_input_embeds[:,0])\n",
    "            cur_input_embeds[img_start_token[batch_idx]:img_start_token[batch_idx]+cur_image_features.shape[0]] =cur_image_features\n",
    "            print(cur_input_embeds[:,0])\n",
    "        else:\n",
    "            cur_input_embeds_1 = model().embed_tokens(cur_input_ids)\n",
    "            cur_input_embeds = torch.cat([cur_input_embeds_1, cur_image_features[0:0]], dim=0)\n",
    "        new_input_embeds.append(cur_input_embeds)\n",
    "        new_labels.append(labels[batch_idx])\n",
    "        cur_image_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
