{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [00:00, 121205.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 19.58888888888888\n",
      "35 10\n",
      "80 80\n",
      "48 47.83\n",
      "time_acc: 0.5294, weather_acc: 0.2857, position_acc: 1.0000, battery_acc: 0.9965, total_acc: 0.7871\n",
      "time_acc:tensor([0.5294])\n",
      "weather_acc:tensor([0.2857])\n",
      "position_acc:tensor([1.])\n",
      "battery_acc:tensor([0.9965])\n",
      "total_acc:tensor([0.7871])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "# 定义 color_mapping\n",
    "color_mapping = {\n",
    "    'red': [0.1, 0.15],\n",
    "    'green': [0.15, 0.3],\n",
    "    'blue': [0.3, 0.45],\n",
    "    'yellow': [0.45, 0.6],\n",
    "    'orange': [0.6, 0.75],\n",
    "    'purple': [0.75, 0.9]\n",
    "}\n",
    "\n",
    "# 提取所有颜色\n",
    "colors = color_mapping.keys()\n",
    "\n",
    "# 检查颜色是否存在于字符串中\n",
    "def find_colors_in_string(input_string):\n",
    "    found_colors = [color for color in colors if color in input_string]\n",
    "    return found_colors\n",
    "\n",
    "\n",
    "def map_to_color(pixel):\n",
    "    if pixel<0.1:\n",
    "        return 'black'\n",
    "    elif 0.1<=pixel<0.15:\n",
    "        return 'red'\n",
    "    elif 0.15<=pixel<0.3:\n",
    "        return 'green'\n",
    "    elif 0.3<=pixel<0.45:\n",
    "        return 'blue'\n",
    "    elif 0.45<=pixel<0.6:\n",
    "        return 'yellow'\n",
    "    elif 0.6<=pixel<0.75:\n",
    "        return 'orange'\n",
    "    elif 0.75<=pixel<=0.9:\n",
    "        return 'purple'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "def compare_img(gen_img,gt_img):\n",
    "    correct_pixel=0\n",
    "    incorrect_pixel=0\n",
    "    for i in range(len(gen_img)):\n",
    "        for j in range(len(gen_img[i])):\n",
    "            if map_to_color(gen_img[i][j])!=map_to_color(gt_img[i][j]):\n",
    "                incorrect_pixel+=1\n",
    "            else:\n",
    "                correct_pixel+=1\n",
    "    return correct_pixel,incorrect_pixel\n",
    "\n",
    "acc=[]\n",
    "answer_list=[i*30 for i in range(10,11)]\n",
    "for answer in answer_list:\n",
    "    #data_path=f'/datadrive_a/jihai/LLaVA/answer/answer-llava-v1.5-7b-mix-u-odd-{i}.jsonl'\n",
    "    data_path=f'./answer/answer-llava-v1.5-7b-siglip-vq_u_weather_biased-2-u-sw-lora-{answer}.jsonl'\n",
    "    time_count=0\n",
    "    time_score=0\n",
    "    weather_count=0\n",
    "    weather_score=0\n",
    "    position_count=0\n",
    "    position_score=0\n",
    "    battery_count=0\n",
    "    battery_score=0\n",
    "    with open (data_path, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            json_obj = json.loads(line.strip())\n",
    "            ground_truth=json_obj['groun_truth']\n",
    "            answer=json_obj['answer']\n",
    "            prompt=json_obj['prompt']\n",
    "            if prompt[0]!='<':\n",
    "                continue\n",
    "            if ' ' not in ground_truth:\n",
    "                if ':' in ground_truth:\n",
    "                    time_count+=1\n",
    "                    pattern = r\"(\\d{2}):(\\d{2}):(\\d{2})\"\n",
    "                    match = re.search(pattern, ground_truth)\n",
    "                    gt_h = int(match.group(1))\n",
    "                    gt_m = int(match.group(2))\n",
    "                    gt_s = int(match.group(3))\n",
    "                    match = re.search(pattern, answer)\n",
    "                    if match:\n",
    "                        ans_h = int(match.group(1))\n",
    "                        ans_m = int(match.group(2))\n",
    "                        ans_s = int(match.group(3))\n",
    "                        err_h=abs(ans_h - gt_h)\n",
    "                        err_h=min(err_h,12-err_h)\n",
    "                        err_m=abs(ans_m - gt_m)\n",
    "                        err_m=min(err_m,60-err_m)\n",
    "                        err_s=abs(ans_s - gt_s)\n",
    "                        err_s=min(err_s,60-err_s)\n",
    "                        err=(err_h/6.0 + err_m/30.0 + err_s/30.0)/3.0\n",
    "    \n",
    "                        time_score+=1-err\n",
    "                elif 'sunny' in ground_truth or 'raining' in ground_truth or 'cloudy' in ground_truth:\n",
    "                    weather_count+=1\n",
    "                    if ground_truth in answer:\n",
    "                        weather_score+=1\n",
    "                elif '-' in ground_truth:\n",
    "                    position_count+=1\n",
    "                    if ground_truth in answer:\n",
    "                        position_score+=1\n",
    "                elif '%' in ground_truth:\n",
    "                    battery_count+=1\n",
    "                    match = re.search(r'\\b(100|[1-9]\\d?|0)%', ground_truth)\n",
    "                    gt=int(match.group(1)) / 100\n",
    "                    match = re.search(r'\\b(100|[1-9]\\d?|0)%', answer)\n",
    "                    if match:\n",
    "                        ans=int(match.group(1)) / 100\n",
    "                        err=abs(ans - gt)\n",
    "                        battery_score+=1-err\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown ground truth format: {ground_truth}\")\n",
    "    time_acc=time_score/time_count\n",
    "    weather_acc=weather_score/weather_count\n",
    "    position_acc=position_score/position_count\n",
    "    battery_acc=battery_score/battery_count\n",
    "    print(time_count, time_score)\n",
    "    print(weather_count, weather_score)\n",
    "    print(position_count, position_score)\n",
    "    print(battery_count, battery_score)\n",
    "    total_acc=(time_score+weather_score+position_score+battery_score)/(time_count+weather_count+position_count+battery_count)\n",
    "    acc.append([time_acc,weather_acc,position_acc,battery_acc,total_acc])\n",
    "    print(f\"time_acc: {time_acc:.4f}, weather_acc: {weather_acc:.4f}, position_acc: {position_acc:.4f}, battery_acc: {battery_acc:.4f}, total_acc: {total_acc:.4f}\")\n",
    "acc=torch.Tensor(acc)\n",
    "acc=acc.permute(1,0)\n",
    "print(f\"time_acc:{acc[0]}\")\n",
    "print(f\"weather_acc:{acc[1]}\")\n",
    "print(f\"position_acc:{acc[2]}\")\n",
    "print(f\"battery_acc:{acc[3]}\")\n",
    "print(f\"total_acc:{acc[4]}\")\n",
    "\n",
    "columns = [f\"{i*10}%\" for i in range(1, 1+acc.shape[1])]\n",
    "row_index = ['time_acc', 'weather_acc', 'position_acc', 'battery_acc', 'total_acc']\n",
    "df = pd.DataFrame(data=acc.numpy(),\n",
    "                  index=row_index,\n",
    "                  columns=columns)\n",
    "df.to_csv(\"output.csv\", index=True, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.963\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9922455089820358\n"
     ]
    }
   ],
   "source": [
    "time_count=35\n",
    "weather_count=31\n",
    "position_count=61\n",
    "battery_count=40\n",
    "time_score=0.963*35\n",
    "weather_score=1*31\n",
    "position_score=1*61\n",
    "battery_score=1*40\n",
    "\n",
    "time_acc=time_score/time_count\n",
    "weather_acc=weather_score/weather_count\n",
    "position_acc=position_score/position_count\n",
    "battery_acc=battery_score/battery_count\n",
    "print(time_acc)\n",
    "print(weather_acc)\n",
    "print(position_acc)\n",
    "print(battery_acc)\n",
    "total_acc=(time_score+weather_score+position_score+battery_score)/(time_count+weather_count+position_count+battery_count)\n",
    "print(total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path='/public_data/jihai/data/multimodalout/smart_watch_train_120ku_180km.json'\n",
    "data_list=json.load(open(data_path, \"r\"))\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m battery_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m (data_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(f):\n\u001b[1;32m     23\u001b[0m         json_obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     24\u001b[0m         ground_truth\u001b[38;5;241m=\u001b[39mjson_obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroun_truth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont,ImageColor\n",
    "\n",
    "# sample=torch.load('/public_data/jihai/understanding/scripts/v1_5/answer/answer-llava-v1.5-7b-siglip-vq-sw-lora-450-image/11.pt')\n",
    "# print(sample.shape)\n",
    "# plt.imshow(sample)\n",
    "\n",
    "gt_image_folder='/public_data/jihai/data/multimodalout/smart_watch_image_test'\n",
    "\n",
    "answer_list=[i*45 for i in range(1,18)]\n",
    "answer=answer_list[-1]\n",
    "#data_path=f'/datadrive_a/jihai/LLaVA/answer/answer-llava-v1.5-7b-mix-u-odd-{i}.jsonl'\n",
    "data_path=f'./answer/answer-llava-v1.5-7b-vq-vq_120ku_180km-2-sw-lora-{answer}.jsonl'\n",
    "count=0\n",
    "time_count=0\n",
    "weather_count=0\n",
    "position_count=0\n",
    "battery_count=0\n",
    "with open (data_path, \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        json_obj = json.loads(line.strip())\n",
    "        ground_truth=json_obj['groun_truth']\n",
    "        answer=json_obj['answer']\n",
    "        prompt=json_obj['prompt']\n",
    "        if prompt[0]=='<':\n",
    "            continue\n",
    "        if ':' in prompt:\n",
    "            time_count+=1\n",
    "            # pattern = r\"(\\d{2}):(\\d{2}):(\\d{2})\"\n",
    "            # match = re.search(pattern, prompt)\n",
    "            # gt_h = int(match.group(1))\n",
    "            # gt_m = int(match.group(2))\n",
    "            # gt_s = int(match.group(3))\n",
    "           \n",
    "        if 'sunny' in prompt or 'raining' in prompt or 'cloudy' in prompt:\n",
    "            weather_count+=1\n",
    "            \n",
    "        if '-' in prompt:\n",
    "            position_count+=prompt.count('-')\n",
    "           \n",
    "        if '%' in prompt:\n",
    "            battery_count+=1\n",
    "            match = re.search(r'\\b(100|[1-9]\\d?|0)%', prompt)\n",
    "            gt=int(match.group(1)) / 100\n",
    "            \n",
    "        gt_image_path=os.path.join(gt_image_folder,json_obj['groun_truth_img_tensor'])\n",
    "        gt_image=Image.open(gt_image_path)\n",
    "        gen_image=torch.load(json_obj['output_img_file'])\n",
    "        \n",
    "        # plt.title(prompt)\n",
    "        # plt.imshow(gt_image)\n",
    "        # plt.imshow(gen_image)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # figsize 可根据需要调整\n",
    "\n",
    "        # 设置整个图的标题\n",
    "        # fig.suptitle(prompt, fontsize=16)  # 总标题\n",
    "        print(prompt)\n",
    "\n",
    "\n",
    "        # 左边子图：显示 gt_image\n",
    "        axes[0].imshow(gt_image)\n",
    "        axes[0].set_title(\"Ground Truth Image\")  # 子图标题\n",
    "        axes[0].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 右边子图：显示 gen_image\n",
    "        axes[1].imshow(gen_image)\n",
    "        axes[1].set_title(\"Generated Image\")  # 子图标题\n",
    "        axes[1].axis('off')  # 关闭坐标轴\n",
    "\n",
    "        # 调整布局以避免重叠\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.9])  # 为总标题留出空间\n",
    "\n",
    "        # 显示图像\n",
    "        plt.savefig(f'./calculate/{count}.png')\n",
    "        plt.close(fig)\n",
    "        count+=1\n",
    "        if count==30:\n",
    "            break\n",
    "print(f\"time_count:{time_count}\")\n",
    "print(f\"weather_count:{weather_count}\")\n",
    "print(f\"position_count:{position_count}\")\n",
    "print(f\"battery_count:{battery_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args: DataArguments):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.list_data_dict = list_data_dict\n",
    "        self.data_args = data_args\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            img_tokens = 128 if 'image' in sample else 0\n",
    "            length_list.append(sum(len(conv['value'].split()) for conv in sample['conversations']) + img_tokens)\n",
    "        return length_list\n",
    "\n",
    "    @property\n",
    "    def modality_lengths(self):\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            cur_len = sum(len(conv['value'].split()) for conv in sample['conversations'])\n",
    "            cur_len = cur_len if 'image' in sample else -cur_len\n",
    "            length_list.append(cur_len)\n",
    "        return length_list\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        assert len(sources) == 1, \"Don't know why it is wrapped to a list\"  # FIXME\n",
    "        if 'image' in sources[0]:\n",
    "            image_file = self.list_data_dict[i]['image']\n",
    "            image_folder = self.data_args.image_folder\n",
    "            processor = self.data_args.image_processor\n",
    "            image = Image.open(os.path.join(image_folder, image_file)).convert('RGB')\n",
    "            if self.data_args.image_aspect_ratio == 'pad':\n",
    "                def expand2square(pil_img, background_color):\n",
    "                    width, height = pil_img.size\n",
    "                    if width == height:\n",
    "                        return pil_img\n",
    "                    elif width > height:\n",
    "                        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "                        result.paste(pil_img, (0, (width - height) // 2))\n",
    "                        return result\n",
    "                    else:\n",
    "                        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "                        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "                        return result\n",
    "                image = expand2square(image, tuple(int(x*255) for x in processor.image_mean))\n",
    "                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "            else:\n",
    "                image = processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "            sources = preprocess_multimodal(\n",
    "                copy.deepcopy([e[\"conversations\"] for e in sources]),\n",
    "                self.data_args)\n",
    "        else:\n",
    "            sources = copy.deepcopy([e[\"conversations\"] for e in sources])\n",
    "        data_dict = preprocess(\n",
    "            sources,\n",
    "            self.tokenizer,\n",
    "            has_image=('image' in self.list_data_dict[i]))\n",
    "        if isinstance(i, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                             labels=data_dict[\"labels\"][0])\n",
    "\n",
    "        # image exist in the data\n",
    "        if 'image' in self.list_data_dict[i]:\n",
    "            data_dict['image'] = image\n",
    "        elif self.data_args.is_multimodal:\n",
    "            # image does not exist in the data, but the model is multimodal\n",
    "            crop_size = self.data_args.image_processor.crop_size\n",
    "            data_dict['image'] = torch.zeros(3, crop_size['height'], crop_size['width'])\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "        batch = dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "        if 'image' in instances[0]:\n",
    "            images = [instance['image'] for instance in instances]\n",
    "            if all(x is not None and x.shape == images[0].shape for x in images):\n",
    "                batch['images'] = torch.stack(images)\n",
    "            else:\n",
    "                batch['images'] = images\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'sa_545504', 'image': 'sam/images/sa_545504.jpg', 'conversations': [{'from': 'human', 'value': \"<image>\\nWhat is this photo about'?\"}, {'from': 'gpt', 'value': \"The image captures a breathtaking view of the Hong Kong skyline at sunset. The sky, awash with hues of orange and blue, serves as a stunning backdrop to the city's architectural marvels. The tallest building, the International Finance Centre, pierces the sky with its imposing height. Its lights are switched on, casting a warm glow that contrasts with the cool tones of the evening sky.\\n\\nThe other buildings, though not as tall, are no less impressive. They are adorned with lights that twinkle like stars against the twilight sky. The water below mirrors the sky's colors, adding to the overall vibrancy of the scene.\\n\\nThe perspective of the image is from the water, looking towards the shore. This viewpoint allows for a comprehensive view of the cityscape, from the towering skyscrapers to the smaller structures nestled among them. The image encapsulates the essence of Hong Kong's urban landscape, a blend of modernity and natural beauty.\"}]}\n"
     ]
    }
   ],
   "source": [
    "list_data_dict = json.load(open('/public_data/ShareGPT4V/sharegpt4v/share-captioner_coco_lcs_sam_1246k_1107.json', \"r\"))\n",
    "print(list_data_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 770485/1246901 [00:00<00:00, 2115173.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reach 200k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "generation_q=[\n",
    "        \"Can you generate a image of the following:\",\n",
    "        \"Can you draw a image of the following:\",\n",
    "        \"Please show a image of the following:\"]\n",
    "\n",
    "generation_a=[\n",
    "        \"Sure, here is the image. \\n<image>\",\n",
    "        \"The image is shown as you wish. \\n<image>\",\n",
    "        \"The image is generated. \\n<image>\"]\n",
    "generation_data = []\n",
    "count=0\n",
    "for data_dict in tqdm(list_data_dict):\n",
    "    if 'sam' in data_dict['image']:\n",
    "        continue\n",
    "    caption=data_dict['conversations'][1]['value']\n",
    "    data_dict['conversations'][0]['value']= f\"{random.choice(generation_q)} {caption}\"\n",
    "    data_dict['conversations'][1]['value']= f\"{random.choice(generation_a)}\"\n",
    "    data_dict['task']='generation'\n",
    "    data_dict['image'] = f\"/public_data/ShareGPT4V/{data_dict['image']}\"\n",
    "    generation_data.append(data_dict)\n",
    "    count+=1\n",
    "    if count>=200000:\n",
    "        print('reach 200k')\n",
    "        break\n",
    "\n",
    "\n",
    "with open('/public_data/jihai/data/share-captioner_coco_lcs_150k_generation.json', \"w\") as f:\n",
    "    json.dump(generation_data, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '000000125860', 'image': '/public_data/ShareGPT4V/coco/train2017/000000125860.jpg', 'conversations': [{'from': 'human', 'value': 'Can you draw a image of the following: In the image, a blue and white bus is parked at a bus stop. The bus is facing towards the right side of the image, ready to embark on its journey. The bus stop is situated on the right side of the image, providing a clear view of the surroundings. A yellow traffic light stands guard on the left side of the image, adding a pop of color to the scene. The sky above is a clear blue, dotted with white clouds, suggesting a bright and sunny day. In the background, trees can be seen, adding a touch of nature to the urban setting.'}, {'from': 'gpt', 'value': 'The image is shown as you wish. \\n<image>'}], 'task': 'generation'}, {'id': '000000571342', 'image': '/public_data/ShareGPT4V/coco/train2017/000000571342.jpg', 'conversations': [{'from': 'human', 'value': \"Please show a image of the following: In the image, a tranquil scene unfolds in front of a white building with a blue door. A brown cow, the main subject of the image, is standing on a concrete platform. The cow is engaged in the act of drinking water from a pink bowl, which is placed on the ground in front of it. The cow's gaze is directed towards the bowl, indicating its focus on the task at hand.\\n\\nTo the right of the cow, a white dog is lying down on the ground. The dog appears to be resting, adding a sense of calmness to the scene. \\n\\nIn the background, two individuals can be seen. One of them is wearing a blue shirt, while the other is in a red shirt. Their presence adds a human element to the otherwise animal-dominated scene.\\n\\nThe image is taken from a slightly elevated perspective, providing a comprehensive view of the scene. The relative positions of the objects and their interactions create a harmonious composition that tells a story of everyday life.\"}, {'from': 'gpt', 'value': 'Sure, here is the image. \\n<image>'}], 'task': 'generation'}, {'id': '000000159356', 'image': '/public_data/ShareGPT4V/coco/train2017/000000159356.jpg', 'conversations': [{'from': 'human', 'value': \"Can you generate a image of the following: The image presents a scene of a workspace. Dominating the scene are two keyboards, one black and one silver, resting on a wooden desk. The black keyboard is positioned on the left side of the desk, while the silver keyboard is on the right. Both keyboards are equipped with black keys and are connected by black cords, suggesting they are ready for use.\\n\\nIn addition to the keyboards, there's a black computer mouse situated on the right side of the desk. The mouse, keyboard, and desk all share the same wooden surface, creating a harmonious workspace.\\n\\nIn the background, there's a hint of human activity. A book and a white box can be seen, adding depth to the scene. The book and box are slightly blurred, indicating they are not the main focus of this image.\\n\\nOverall, the image captures a quiet moment in a workspace, with the black and silver keyboards and black computer mouse waiting patiently for their user.\"}, {'from': 'gpt', 'value': 'Sure, here is the image. \\n<image>'}], 'task': 'generation'}]\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(generation_data[:3])\n",
    "print(len(generation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865298\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "list_data_dict1 = json.load(open('/public_data/jihai/data/share-captioner_coco_lcs_150k_generation.json', \"r\"))\n",
    "list_data_dict2=json.load(open('/public_data/ShareGPT4V/sharegpt4v/llava_v1_5_mix665k.json','r'))\n",
    "list_data_dict=list_data_dict1[:200000]+list_data_dict2\n",
    "print(len(list_data_dict))\n",
    "with open('/public_data/ShareGPT4V/sharegpt4v/llava_v1_5_mix665k_with_generation.json', \"w\") as f:\n",
    "    json.dump(list_data_dict, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "708128\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "list_data_dict1 = json.load(open('/public_data/jihai/data/share-captioner_coco_lcs_150k_generation.json', \"r\"))\n",
    "print(len(list_data_dict1))\n",
    "list_data_dict2=json.load(open('/public_data/ShareGPT4V/sharegpt4v/blip_laion_cc_sbu_558k.json','r'))\n",
    "list_data_dict=list_data_dict1[:150000]+list_data_dict2\n",
    "print(len(list_data_dict))\n",
    "with open('/public_data/ShareGPT4V/sharegpt4v/blip_laion_cc_sbu_558k_with_generation.json', \"w\") as f:\n",
    "    json.dump(list_data_dict, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "708352"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2767*8*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '000000125860', 'image': '/public_data/ShareGPT4V/coco/train2017/000000125860.jpg', 'conversations': [{'from': 'human', 'value': 'Can you draw a image of the following: In the image, a blue and white bus is parked at a bus stop. The bus is facing towards the right side of the image, ready to embark on its journey. The bus stop is situated on the right side of the image, providing a clear view of the surroundings. A yellow traffic light stands guard on the left side of the image, adding a pop of color to the scene. The sky above is a clear blue, dotted with white clouds, suggesting a bright and sunny day. In the background, trees can be seen, adding a touch of nature to the urban setting.'}, {'from': 'gpt', 'value': 'The image is shown as you wish. \\n<image>'}], 'task': 'generation'}, {'id': '000000571342', 'image': '/public_data/ShareGPT4V/coco/train2017/000000571342.jpg', 'conversations': [{'from': 'human', 'value': \"Please show a image of the following: In the image, a tranquil scene unfolds in front of a white building with a blue door. A brown cow, the main subject of the image, is standing on a concrete platform. The cow is engaged in the act of drinking water from a pink bowl, which is placed on the ground in front of it. The cow's gaze is directed towards the bowl, indicating its focus on the task at hand.\\n\\nTo the right of the cow, a white dog is lying down on the ground. The dog appears to be resting, adding a sense of calmness to the scene. \\n\\nIn the background, two individuals can be seen. One of them is wearing a blue shirt, while the other is in a red shirt. Their presence adds a human element to the otherwise animal-dominated scene.\\n\\nThe image is taken from a slightly elevated perspective, providing a comprehensive view of the scene. The relative positions of the objects and their interactions create a harmonious composition that tells a story of everyday life.\"}, {'from': 'gpt', 'value': 'Sure, here is the image. \\n<image>'}], 'task': 'generation'}, {'id': '000000159356', 'image': '/public_data/ShareGPT4V/coco/train2017/000000159356.jpg', 'conversations': [{'from': 'human', 'value': \"Can you generate a image of the following: The image presents a scene of a workspace. Dominating the scene are two keyboards, one black and one silver, resting on a wooden desk. The black keyboard is positioned on the left side of the desk, while the silver keyboard is on the right. Both keyboards are equipped with black keys and are connected by black cords, suggesting they are ready for use.\\n\\nIn addition to the keyboards, there's a black computer mouse situated on the right side of the desk. The mouse, keyboard, and desk all share the same wooden surface, creating a harmonious workspace.\\n\\nIn the background, there's a hint of human activity. A book and a white box can be seen, adding depth to the scene. The book and box are slightly blurred, indicating they are not the main focus of this image.\\n\\nOverall, the image captures a quiet moment in a workspace, with the black and silver keyboards and black computer mouse waiting patiently for their user.\"}, {'from': 'gpt', 'value': 'Sure, here is the image. \\n<image>'}], 'task': 'generation'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "list_data_dict = json.load(open('./blip_laion_cc_sbu_558k_with_generation.json', \"r\"))\n",
    "print(list_data_dict[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
