{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from email.mime import image\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_main:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda:7'\n",
    "        self.ckpt_start = 10\n",
    "        self.ckpt_step = 30\n",
    "        self.ckpt_num = 1\n",
    "        self.model_name = 'llava-v1.5-7b'\n",
    "        self.understanding_only = False\n",
    "        self.generation_only = False\n",
    "args = Args_main()\n",
    "\n",
    "#load trained model\n",
    "device=args.device\n",
    "ckp_list=[i*args.ckpt_step for i in range(args.ckpt_start,args.ckpt_num+args.ckpt_start)]\n",
    "model_name=args.model_name\n",
    "understanding_only=args.understanding_only\n",
    "generation_only=args.generation_only\n",
    "model_list=[f'/public_data/jihai/understanding/scripts/v1_5/checkpoints/{model_name}/checkpoint-{i}' for i in ckp_list]\n",
    "k=0\n",
    "infer_args = type('Args', (), {\n",
    "    \"model_path\": '/public_data/jihai/understanding/scripts/v1_5/checkpoints/llava-v1.5-7b/checkpoint-6761',\n",
    "    \"model_base\": None,\n",
    "    \"data_path\": '/public_data/jihai/data/multimodalout/smart_watch_image_train_weather_tl.json',\n",
    "    \"image_folder\": '/public_data/jihai/data/multimodalout/smart_watch_image_train_weather_tl',\n",
    "    \"conv_mode\": \"llava_v1\",\n",
    "    \"num_chunks\": 1,\n",
    "    \"chunk_idx\": 0,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"image_un_size\": [3,224,224],\n",
    "    \"image_gen_size\": [3,256,256]\n",
    "})()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/public_data/jihai/understanding/scripts/v1_5/checkpoints/llava-v1.5-7b/checkpoint-6761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:24<00:00,  8.23s/it]\n",
      "Some weights of the model checkpoint at /public_data/jihai/understanding/scripts/v1_5/checkpoints/llava-v1.5-7b/checkpoint-6761 were not used when initializing LlavaLlamaForCausalLM_ImgGen: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.k.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.k.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.norm.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.norm.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.proj_out.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.proj_out.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.q.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.q.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.v.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.0.v.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.k.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.k.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.norm.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.norm.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.proj_out.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.proj_out.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.q.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.q.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.v.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.1.v.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.k.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.k.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.norm.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.norm.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.proj_out.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.proj_out.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.q.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.q.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.v.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.attn.2.v.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.2.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.2.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.2.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.2.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.2.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.2.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.2.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.res.2.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.upsample.conv.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.0.upsample.conv.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.nin_shortcut.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.nin_shortcut.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.2.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.2.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.2.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.2.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.2.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.2.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.2.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.res.2.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.upsample.conv.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.1.upsample.conv.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.2.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.2.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.2.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.2.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.2.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.2.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.2.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.res.2.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.upsample.conv.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.2.upsample.conv.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.nin_shortcut.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.nin_shortcut.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.2.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.2.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.2.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.2.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.2.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.2.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.2.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.res.2.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.upsample.conv.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.3.upsample.conv.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.2.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.2.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.2.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.2.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.2.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.2.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.2.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_blocks.4.res.2.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_in.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_in.weight', 'model.vision_tower_gen.vision_tower.decoder.conv_out.bias', 'model.vision_tower_gen.vision_tower.decoder.conv_out.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.0.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.0.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.0.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.0.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.0.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.0.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.0.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.0.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.1.k.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.1.k.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.1.norm.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.1.norm.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.1.proj_out.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.1.proj_out.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.1.q.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.1.q.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.1.v.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.1.v.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.2.conv1.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.2.conv1.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.2.conv2.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.2.conv2.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.2.norm1.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.2.norm1.weight', 'model.vision_tower_gen.vision_tower.decoder.mid.2.norm2.bias', 'model.vision_tower_gen.vision_tower.decoder.mid.2.norm2.weight', 'model.vision_tower_gen.vision_tower.decoder.norm_out.bias', 'model.vision_tower_gen.vision_tower.decoder.norm_out.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.downsample.conv.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.downsample.conv.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.0.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.downsample.conv.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.downsample.conv.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.1.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.downsample.conv.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.downsample.conv.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.nin_shortcut.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.nin_shortcut.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.2.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.downsample.conv.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.downsample.conv.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.3.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.k.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.k.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.norm.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.norm.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.proj_out.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.proj_out.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.q.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.q.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.v.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.0.v.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.k.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.k.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.norm.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.norm.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.proj_out.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.proj_out.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.q.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.q.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.v.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.attn.1.v.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.nin_shortcut.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.nin_shortcut.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.0.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.1.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.1.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.1.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.1.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.1.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.1.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.1.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_blocks.4.res.1.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_in.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_in.weight', 'model.vision_tower_gen.vision_tower.encoder.conv_out.bias', 'model.vision_tower_gen.vision_tower.encoder.conv_out.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.0.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.0.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.0.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.0.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.0.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.0.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.0.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.0.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.1.k.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.1.k.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.1.norm.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.1.norm.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.1.proj_out.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.1.proj_out.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.1.q.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.1.q.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.1.v.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.1.v.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.2.conv1.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.2.conv1.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.2.conv2.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.2.conv2.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.2.norm1.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.2.norm1.weight', 'model.vision_tower_gen.vision_tower.encoder.mid.2.norm2.bias', 'model.vision_tower_gen.vision_tower.encoder.mid.2.norm2.weight', 'model.vision_tower_gen.vision_tower.encoder.norm_out.bias', 'model.vision_tower_gen.vision_tower.encoder.norm_out.weight', 'model.vision_tower_gen.vision_tower.post_quant_conv.bias', 'model.vision_tower_gen.vision_tower.post_quant_conv.weight', 'model.vision_tower_gen.vision_tower.quant_conv.bias', 'model.vision_tower_gen.vision_tower.quant_conv.weight', 'model.vision_tower_gen.vision_tower.quantize.codebook_used', 'model.vision_tower_gen.vision_tower.quantize.embedding.weight']\n",
      "- This IS expected if you are initializing LlavaLlamaForCausalLM_ImgGen from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaLlamaForCausalLM_ImgGen from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "disable_torch_init()\n",
    "model_path = os.path.expanduser(infer_args.model_path)\n",
    "print(model_path)\n",
    "\n",
    "model_type = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor,image_processor_gen, context_len = load_pretrained_model(model_path,None, model_name,device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0063, -0.0142, -0.0005,  ...,  0.0067,  0.0151, -0.0050],\n",
      "       device='cuda:7', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
      "tensor([-0.0131,  0.0273, -0.0209,  ..., -0.0432, -0.0188, -0.0280],\n",
      "       device='cuda:7', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0256, -0.2451, -0.2129,  0.1982,  0.0732, -0.1445,  0.1816, -0.1826],\n",
      "       device='cuda:7', dtype=torch.float16, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.get_model().mm_projector_head.weight[0])\n",
    "print(model.get_model().mm_projector_un[0].weight[0])\n",
    "print(model.get_model().mm_projector_gen[0].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/112 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:05<00:00, 18.95it/s]\n"
     ]
    }
   ],
   "source": [
    "infer_args.gen_processor=image_processor_gen\n",
    "infer_args.un_processor=image_processor\n",
    "\n",
    "if 'plain' in model_type and 'finetune' not in model_type.lower() and 'mmtag' not in infer_args.conv_mode:\n",
    "    infer_args.conv_mode = infer_args.conv_mode + '_mmtag'\n",
    "    print(f'It seems that this is a plain model, but it is not using a mmtag prompt, auto switching to {infer_args.conv_mode}.')\n",
    "data_loader,data_set = create_data_loader(infer_args, tokenizer, model.config)\n",
    "print (len(data_set))\n",
    "image_features=[]\n",
    "labels=[]\n",
    "for image, w_label in tqdm(data_loader):\n",
    "    image_feature=model.get_model().get_vision_tower()(image.to(device).half())\n",
    "   \n",
    "    if isinstance(image_feature, tuple): #VQ\n",
    "        image_feature = image_feature[0]\n",
    "        image_feature=image_feature.permute(0,2,3,1) #b,h,w,c\n",
    "        image_feature=image_feature.view(image_feature.shape[0],-1,image_feature.shape[-1]) #b,seq_len,c\n",
    "    \n",
    "    #image_feature=image_feature[0]\n",
    "   \n",
    "    image_feature=image_feature[:,0,:]\n",
    "    image_feature=model.get_model().mm_projector_un(image_feature)\n",
    "    image_feature=image_feature.cpu()\n",
    "    image_features.append(image_feature)\n",
    "    labels.append(w_label)\n",
    "image_features=torch.cat(image_features,dim=0)\n",
    "labels=torch.cat(labels, dim=0)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(image_features,'./eval/weather_features_siglip-u.pt')\n",
    "torch.save(labels,'./eval/weather_labels_siglip-u.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "image_features=torch.load('./eval/weather_features_siglip-siglip.pt')\n",
    "labels=torch.load('./eval/weather_labels_siglip-siglip.pt')\n",
    "# 假设你的数据为PyTorch张量，需要先转换为numpy数组\n",
    "# 转换特征数据 [5585, 4096]\n",
    "features = image_features.detach().cpu().numpy()  # 如果已经在CPU可省略.detach().cpu()\n",
    "# 转换标签数据 [5585,]\n",
    "labels = labels.detach().cpu().numpy()\n",
    "\n",
    "# 数据预处理\n",
    "# 1. 标准化处理（重要！因为t-SNE对尺度敏感）\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "#features_scaled=features\n",
    "\n",
    "# 2. 可选：先用PCA降维到50维加速计算（推荐处理高维数据）\n",
    "pca = PCA(n_components=50)\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# 执行t-SNE降维（推荐先PCA再t-SNE）\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, \n",
    "            n_iter=1000, random_state=42)\n",
    "tsne_results = tsne.fit_transform(features_pca)  # 直接使用原始特征可替换为features_scaled\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(4, 4))\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels_dict = {0: 'Sunny', 1: 'Rainy', 2: 'Cloudy'}\n",
    "\n",
    "for i in [0, 1, 2]:\n",
    "    plt.scatter(tsne_results[labels == i, 0], \n",
    "                tsne_results[labels == i, 1], \n",
    "                c=colors[i], \n",
    "                label=labels_dict[i],\n",
    "                alpha=0.6,\n",
    "                s=10)  # 点的大小可根据需要调整\n",
    "\n",
    "#plt.title('t-SNE Visualization of Image Features', fontsize=14)\n",
    "#plt.xlabel('t-SNE Component 1', fontsize=12)\n",
    "#plt.ylabel('t-SNE Component 2', fontsize=12)\n",
    "ax = plt.gca()\n",
    "ax.axis('off')\n",
    "#plt.legend(fontsize=16,markerscale=2)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./eval/weather_features_siglip-siglip.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 0.0107 | Train Acc: 99.27%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 2/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 3/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 4/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 5/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 6/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 7/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 8/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 9/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 10/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 11/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 12/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 13/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 14/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 15/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 16/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 17/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 18/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 19/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 20/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0000 | Train Acc: 100.00%\n",
      "Test Loss: 0.0000 | Test Acc: 100.00%\n",
      "Best Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "image_features=torch.load('./eval/weather_features_vq-u.pt')\n",
    "labels=torch.load('./eval/weather_labels_vq-u.pt')\n",
    "# 假设你的数据为PyTorch张量，需要先转换为numpy数组\n",
    "# 转换特征数据 [5585, 4096]\n",
    "features = image_features.detach().cpu().numpy()  # 如果已经在CPU可省略.detach().cpu()\n",
    "# 转换标签数据 [5585,]\n",
    "labels = labels.detach().cpu().numpy()\n",
    "# 假设原始数据已经转换为numpy数组\n",
    "# features: [5585, 4096]\n",
    "# labels: [5585,]\n",
    "\n",
    "# 1. 数据预处理\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)  # 使用之前t-SNE相同的标准化参数\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "X = torch.tensor(features_scaled, dtype=torch.float32)\n",
    "y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# 创建数据集\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# 划分数据集（4500训练，1085测试）\n",
    "train_size = 4500\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# 2. 定义线性分类器\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LinearProbe(4096, 3).to(device)\n",
    "\n",
    "# 3. 训练配置\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# 4. 训练循环\n",
    "num_epochs = 50\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += targets.size(0)\n",
    "            test_correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    # 打印结果\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss/len(train_loader):.4f} | Train Acc: {100*correct/total:.2f}%\")\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader):.4f} | Test Acc: {100*test_correct/test_total:.2f}%\")\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    current_acc = test_correct / test_total\n",
    "    if current_acc > best_acc:\n",
    "        best_acc = current_acc\n",
    "        torch.save(model.state_dict(), 'best_linear_probe.pth')\n",
    "\n",
    "print(f\"Best Test Accuracy: {100*best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 10%|▉         | 58/600 [02:06<19:43,  2.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m output_ids\u001b[38;5;241m=\u001b[39moutput_ids[:,\u001b[38;5;241m1\u001b[39m:start_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     43\u001b[0m input_ids\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((input_ids, output_ids), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m img_id\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_image_vq\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_tower_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_patches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     46\u001b[0m     img\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mvision_tower_gen\u001b[38;5;241m.\u001b[39mvision_tower\u001b[38;5;241m.\u001b[39mdecode_code(img_id,[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mgenerate_image_vq\u001b[0;34m(input_ids, model, num_image_tokens)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_image_tokens):\n\u001b[0;32m----> 6\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m         img_logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mmm_projector_head(hidden_states[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:])\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:798\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:706\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    703\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[1;32m    704\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, seq_len\u001b[38;5;241m=\u001b[39mkv_seq_len)\n\u001b[0;32m--> 706\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos}  \u001b[38;5;66;03m# Specific to RoPE models\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:234\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    232\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    233\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 234\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m)\n\u001b[1;32m    235\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "images_gen_pad=torch.zeros([0]+infer_args.image_gen_size).to(device=device, dtype=torch.float16)\n",
    "images_un_pad=torch.zeros([0]+infer_args.image_un_size).to(device=device, dtype=torch.float16)\n",
    "count=0\n",
    "for (input_ids, images), line in tqdm(zip(data_loader, list_data_dict), total=len(list_data_dict)):\n",
    "    count+=1\n",
    "    if count==500: break\n",
    "\n",
    "    cur_prompt = line[\"conversations\"][0][\"value\"]\n",
    "    groun_truth=line[\"conversations\"][1][\"value\"]\n",
    "    groun_truth_img_tensor=line[\"image\"]\n",
    "    input_ids = input_ids.to(device=device, non_blocking=True)\n",
    "    images['images_gen']=images['images_gen'].to(dtype=torch.float16, device=device, non_blocking=True) if images['images_gen'] is not None else images_gen_pad\n",
    "    images['images_un']=images['images_un'].to(dtype=torch.float16, device=device, non_blocking=True) if images['images_un'] is not None else images_un_pad\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            images=images,\n",
    "            do_sample=True if infer_args.temperature > 0 else False,\n",
    "            temperature=infer_args.temperature,\n",
    "            top_p=infer_args.top_p,\n",
    "            num_beams=infer_args.num_beams,\n",
    "            max_new_tokens=infer_args.max_new_tokens,\n",
    "            use_cache=True)\n",
    "    output_ids=outputs['generated_tokens']\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=False)[0].strip()\n",
    "    #print(outputs)\n",
    "\n",
    "    img_indicator = torch.tensor([529,  3027, 29958])\n",
    "    id_seq = output_ids[0].cpu()\n",
    "\n",
    "    # 子序列长度\n",
    "    sub_seq_len = len(img_indicator)\n",
    "\n",
    "    # 滑动窗口查找子序列\n",
    "    start_idx = -1\n",
    "    for i in range(id_seq.size(0) - sub_seq_len + 1):\n",
    "        if torch.equal(id_seq[i:i + sub_seq_len], img_indicator):\n",
    "            start_idx = i\n",
    "            break\n",
    "    img_file=None\n",
    "    if start_idx != -1:\n",
    "        output_ids=output_ids[:,1:start_idx+3]\n",
    "        input_ids=torch.cat((input_ids, output_ids), dim=1)\n",
    "        img_id=generate_image_vq(input_ids,model,model.get_model().vision_tower_gen.num_patches)\n",
    "        with torch.no_grad():\n",
    "            img=model.get_model().vision_tower_gen.vision_tower.decode_code(img_id,[1,8,16,16])\n",
    "        img = F.interpolate(img, size=[infer_args.image_gen_size[1], infer_args.image_gen_size[2]], mode='bicubic').permute(0, 2, 3, 1)[0]\n",
    "        img = torch.clamp(127.5 * img + 128.0, 0, 255).to(\"cpu\", dtype=torch.uint8)\n",
    "        img_file=os.path.join(infer_args.answer_image_file, f'{count}.pt')\n",
    "        torch.save(img, img_file)\n",
    "\n",
    "    ans_file.write(json.dumps({\"prompt\": cur_prompt,\n",
    "                                \"groun_truth\": groun_truth,\n",
    "                                \"answer\": outputs,\n",
    "                                \"groun_truth_img_tensor\": groun_truth_img_tensor,\n",
    "                                \"output_img_file\": img_file,\n",
    "                                \"model_id\": model_name,\n",
    "                                \"metadata\": {}}) + \"\\n\")\n",
    "    #outputs = tokenizer.batch_decode(input_ids, skip_special_tokens=False)[0].strip()\n",
    "\n",
    "print(ans_file)\n",
    "ans_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1568/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
