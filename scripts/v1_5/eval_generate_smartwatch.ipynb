{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析命令行参数\n",
    "# parser = argparse.ArgumentParser(description=\"Run model evaluation with configurable parameters.\")\n",
    "# parser.add_argument('--device', type=str, default='cuda:7', help='Device to use (default: cuda:7)')\n",
    "# parser.add_argument('--ckpt_start', type=int, default=1, help='Start multiplier for checkpoints (default: 1)')\n",
    "# parser.add_argument('--ckpt_step', type=int, default=30, help='Step multiplier for checkpoints (default: 30)')\n",
    "# parser.add_argument('--ckpt_num', type=int, default=10, help='Number of checkpoints (default: 10)')\n",
    "# parser.add_argument('--model_name', type=str, default='llava-v1.5-7b-sw-u-lora', help='Model name (default: llava-v1.5-7b-sw-u-lora)')\n",
    "# parser.add_argument('--understanding_only', action='store_true', default=False, help='Enable understanding only mode (default: True)')\n",
    "# parser.add_argument('--generation_only', action='store_true', default=False, help='Enable generation only mode (default: False)')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from email.mime import image\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shortuuid\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "\n",
    "def split_list(lst, n):\n",
    "    chunk_size = math.ceil(len(lst) / n)  # integer division\n",
    "    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "def get_chunk(lst, n, k):\n",
    "    chunks = split_list(lst, n)\n",
    "    return chunks[k]\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,args, tokenizer, model_config,generation_only=False, understanding_only=False):\n",
    "        self.list_data_dict = json.load(open(args.data_path, \"r\"))\n",
    "        if generation_only:\n",
    "            self.list_data_dict = [e for e in self.list_data_dict if e['task']==\"generation\"]\n",
    "        if understanding_only:\n",
    "            self.list_data_dict = [e for e in self.list_data_dict if (e['task']==\"vqa\" or e['task']==\"caption\")]\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_config = model_config\n",
    "        self.image_folder = args.image_folder\n",
    "        self.gen_processor=args.gen_processor\n",
    "        self.un_processor=args.un_processor\n",
    "        self.conv_mode=args.conv_mode\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sources = self.list_data_dict[index]\n",
    "        \n",
    "        qs = sources[\"conversations\"][0][\"value\"]\n",
    "        # if self.model_config.mm_use_im_start_end:\n",
    "        #     qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n",
    "        # else:\n",
    "        #     qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n",
    "        \n",
    "\n",
    "        conv = conv_templates[self.conv_mode].copy()\n",
    "        conv.append_message(conv.roles[0], qs)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "        #print(prompt)\n",
    "        image_file = sources[\"image\"]\n",
    "        image = Image.open(os.path.join(self.image_folder, image_file)).convert('RGB')\n",
    "        image_un=None\n",
    "        image_gen=None\n",
    "        if sources['task']=='generation':\n",
    "            image_gen = self.gen_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "        else:\n",
    "            image_un = self.un_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n",
    "        input_ids = tokenizer_image_token(prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt')\n",
    "\n",
    "        return input_ids, image_un,image_gen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids, image_un,image_gen = zip(*batch)\n",
    "    image_un= [img for img in image_un if img is not None]\n",
    "    image_gen= [img for img in image_gen if img is not None]\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    image_un = torch.stack(image_un, dim=0) if len(image_un) > 0 else None\n",
    "    image_gen = torch.stack(image_gen, dim=0) if len(image_gen) > 0 else None\n",
    "    images={'images_un':image_un,'images_gen':image_gen}\n",
    "    return input_ids, images\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "def create_data_loader(args, tokenizer, model_config, batch_size=1, num_workers=4,understanding_only=False,generation_only=False):\n",
    "    assert batch_size == 1, \"batch_size must be 1\"\n",
    "    dataset = CustomDataset(args, tokenizer, model_config,understanding_only=understanding_only,generation_only=generation_only)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, collate_fn=collate_fn)\n",
    "    return data_loader,dataset\n",
    "\n",
    "def generate_image(input_ids,model,num_image_tokens):\n",
    "    output_img=[]\n",
    "    inputs_embeds=model.get_model().embed_tokens(input_ids) #1, seq_le, 4096\n",
    "    with torch.inference_mode():\n",
    "        for i in range(num_image_tokens):\n",
    "            outputs = model.model(\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                past_key_values=None,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "            hidden_states = outputs[0]\n",
    "            img = model.get_model().mm_projector_head(hidden_states[:,-1,:])\n",
    "            output_img.append(img)\n",
    "            if model.get_model().mm_projector_gen is not None:\n",
    "                new_embed=model.get_model().mm_projector_gen(img)\n",
    "            else:\n",
    "                new_embed=model.get_model().mm_projector_un(img)\n",
    "            new_embed=new_embed.unsqueeze(1).to(inputs_embeds.device)\n",
    "            inputs_embeds=torch.cat([inputs_embeds,new_embed],dim=1)\n",
    "            \n",
    "    return output_img\n",
    "\n",
    "\n",
    "# def generate_image_vq(input_ids,model,num_image_tokens):\n",
    "#     output_img_id=[]\n",
    "#     inputs_embeds=model.get_model().embed_tokens(input_ids) #1, seq_le, 4096\n",
    "#     with torch.inference_mode():\n",
    "#         for i in range(num_image_tokens):\n",
    "#             outputs = model.model(\n",
    "#                 input_ids=None,\n",
    "#                 attention_mask=None,\n",
    "#                 position_ids=None,\n",
    "#                 past_key_values=None,\n",
    "#                 inputs_embeds=inputs_embeds,\n",
    "#             )\n",
    "#             hidden_states = outputs[0]\n",
    "#             img_logits = model.get_model().mm_projector_head(hidden_states[:,-1,:])\n",
    "#             img_id=img_logits.argmax(dim=-1).item()\n",
    "#             output_img_id.append(img_id)\n",
    "#             img_latent=model.get_model().vision_tower.quantize.get_codebook_entry(img_id, shape=None, channel_first=True)\n",
    "#             if model.get_model().mm_projector_gen is not None:\n",
    "#                 new_embed=model.get_model().mm_projector_gen(img_latent)\n",
    "#             else:\n",
    "#                 new_embed=model.get_model().mm_projector_un(img_latent)\n",
    "#             new_embed=new_embed.unsqueeze(1).to(inputs_embeds.device)\n",
    "#             inputs_embeds=torch.cat([inputs_embeds,new_embed],dim=1)\n",
    "            \n",
    "#     return output_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_main:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda:7'\n",
    "        self.ckpt_start = 1\n",
    "        self.ckpt_step = 70\n",
    "        self.ckpt_num = 10\n",
    "        self.model_name = 'llava-v1.5-7b-siglip-vq-sw-lora'\n",
    "        self.understanding_only = False\n",
    "        self.generation_only = False\n",
    "args = Args_main()\n",
    "\n",
    "#load trained model\n",
    "device=args.device\n",
    "ckp_list=[i*args.ckpt_step for i in range(args.ckpt_start,args.ckpt_num+args.ckpt_start)]\n",
    "model_name=args.model_name\n",
    "understanding_only=args.understanding_only\n",
    "generation_only=args.generation_only\n",
    "model_list=[f'/public_data/jihai/understanding/scripts/v1_5/checkpoints/{model_name}/checkpoint-{i}' for i in ckp_list]\n",
    "k=9\n",
    "infer_args = type('Args', (), {\n",
    "    \"model_path\": model_list[k],\n",
    "    \"model_base\": '/public_data/jihai/tmp/vicuna-7b-v1.5',\n",
    "    \"data_path\": '/public_data/jihai/data/multimodalout/smart_watch_test.json',\n",
    "    \"image_folder\": '/public_data/jihai/data/multimodalout/smart_watch_image_test',\n",
    "    \"answers_file\": f\"./answer/answer-{model_name}-{ckp_list[k]}.jsonl\",\n",
    "    \"answer_image_file\": f\"./answer/answer-{model_name}-{ckp_list[k]}-image\",\n",
    "    \"conv_mode\": \"llava_v1\",\n",
    "    \"num_chunks\": 1,\n",
    "    \"chunk_idx\": 0,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": None,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"image_un_size\": [3,224,224],\n",
    "    \"image_gen_size\": [3,256,256]\n",
    "})()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.32s/it]\n",
      "Some weights of LlavaLlamaForCausalLM_ImgGen were not initialized from the model checkpoint at /public_data/jihai/tmp/vicuna-7b-v1.5 and are newly initialized: ['model.mm_projector_gen.0.bias', 'model.mm_projector_gen.0.weight', 'model.mm_projector_gen.2.bias', 'model.mm_projector_gen.2.weight', 'model.mm_projector_head.bias', 'model.mm_projector_head.weight', 'model.mm_projector_un.0.bias', 'model.mm_projector_un.0.weight', 'model.mm_projector_un.2.bias', 'model.mm_projector_un.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading additional LLaVA weights...\n",
      "Loading LoRA weights...\n",
      "Merging LoRA weights...\n",
      "Model is loaded...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "disable_torch_init()\n",
    "model_path = os.path.expanduser(infer_args.model_path)\n",
    "\n",
    "model_type = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor,image_processor_gen, context_len = load_pretrained_model(model_path, infer_args.model_base, model_name,device=device)\n",
    "infer_args.gen_processor=image_processor_gen\n",
    "infer_args.un_processor=image_processor\n",
    "\n",
    "\n",
    "answers_file = infer_args.answers_file\n",
    "os.makedirs(os.path.dirname(answers_file), exist_ok=True)\n",
    "os.makedirs(infer_args.answer_image_file, exist_ok=True)\n",
    "ans_file = open(answers_file, \"w\")\n",
    "if 'plain' in model_type and 'finetune' not in model_type.lower() and 'mmtag' not in infer_args.conv_mode:\n",
    "    infer_args.conv_mode = infer_args.conv_mode + '_mmtag'\n",
    "    print(f'It seems that this is a plain model, but it is not using a mmtag prompt, auto switching to {infer_args.conv_mode}.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader,data_set = create_data_loader(infer_args, tokenizer, model.config,understanding_only=understanding_only,generation_only=generation_only)\n",
    "list_data_dict = data_set.list_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_vq(input_ids,model,num_image_tokens):\n",
    "    output_img_id=[]\n",
    "    inputs_embeds=model.get_model().embed_tokens(input_ids) #1, seq_le, 4096\n",
    "    with torch.inference_mode():\n",
    "        for i in range(num_image_tokens):\n",
    "            outputs = model.model(\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                position_ids=None,\n",
    "                past_key_values=None,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "            )\n",
    "            hidden_states = outputs[0]\n",
    "            img_logits = model.get_model().mm_projector_head(hidden_states[:,-1,:])\n",
    "            img_id=img_logits.argmax(dim=-1) # shape (1,)\n",
    "            output_img_id.append(img_id)\n",
    "            img_latent=model.get_model().vision_tower_gen.vision_tower.quantize.get_codebook_entry(img_id, shape=None, channel_first=True) # (1,8)\n",
    "            if model.get_model().mm_projector_gen is not None:\n",
    "                new_embed=model.get_model().mm_projector_gen(img_latent)\n",
    "            else:\n",
    "                new_embed=model.get_model().mm_projector_un(img_latent)\n",
    "            new_embed=new_embed.unsqueeze(1).to(inputs_embeds.device)\n",
    "            inputs_embeds=torch.cat([inputs_embeds,new_embed],dim=1)\n",
    "            \n",
    "    return output_img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jihai/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      " 10%|▉         | 58/600 [02:06<19:43,  2.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m output_ids\u001b[38;5;241m=\u001b[39moutput_ids[:,\u001b[38;5;241m1\u001b[39m:start_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     43\u001b[0m input_ids\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((input_ids, output_ids), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m img_id\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_image_vq\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_tower_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_patches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     46\u001b[0m     img\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mvision_tower_gen\u001b[38;5;241m.\u001b[39mvision_tower\u001b[38;5;241m.\u001b[39mdecode_code(img_id,[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36mgenerate_image_vq\u001b[0;34m(input_ids, model, num_image_tokens)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_image_tokens):\n\u001b[0;32m----> 6\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m         img_logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mmm_projector_head(hidden_states[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:])\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:1070\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1061\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1062\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         use_cache,\n\u001b[1;32m   1068\u001b[0m     )\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1070\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1079\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:798\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    807\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:706\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    703\u001b[0m     kv_seq_len \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mget_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx)\n\u001b[1;32m    704\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(value_states, seq_len\u001b[38;5;241m=\u001b[39mkv_seq_len)\n\u001b[0;32m--> 706\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos}  \u001b[38;5;66;03m# Specific to RoPE models\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/multimodal/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:234\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    232\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    233\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin[position_ids]\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 234\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m)\n\u001b[1;32m    235\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "images_gen_pad=torch.zeros([0]+infer_args.image_gen_size).to(device=device, dtype=torch.float16)\n",
    "images_un_pad=torch.zeros([0]+infer_args.image_un_size).to(device=device, dtype=torch.float16)\n",
    "count=0\n",
    "for (input_ids, images), line in tqdm(zip(data_loader, list_data_dict), total=len(list_data_dict)):\n",
    "    count+=1\n",
    "    if count==500: break\n",
    "\n",
    "    cur_prompt = line[\"conversations\"][0][\"value\"]\n",
    "    groun_truth=line[\"conversations\"][1][\"value\"]\n",
    "    groun_truth_img_tensor=line[\"image\"]\n",
    "    input_ids = input_ids.to(device=device, non_blocking=True)\n",
    "    images['images_gen']=images['images_gen'].to(dtype=torch.float16, device=device, non_blocking=True) if images['images_gen'] is not None else images_gen_pad\n",
    "    images['images_un']=images['images_un'].to(dtype=torch.float16, device=device, non_blocking=True) if images['images_un'] is not None else images_un_pad\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            images=images,\n",
    "            do_sample=True if infer_args.temperature > 0 else False,\n",
    "            temperature=infer_args.temperature,\n",
    "            top_p=infer_args.top_p,\n",
    "            num_beams=infer_args.num_beams,\n",
    "            max_new_tokens=infer_args.max_new_tokens,\n",
    "            use_cache=True)\n",
    "    output_ids=outputs['generated_tokens']\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=False)[0].strip()\n",
    "    #print(outputs)\n",
    "\n",
    "    img_indicator = torch.tensor([529,  3027, 29958])\n",
    "    id_seq = output_ids[0].cpu()\n",
    "\n",
    "    # 子序列长度\n",
    "    sub_seq_len = len(img_indicator)\n",
    "\n",
    "    # 滑动窗口查找子序列\n",
    "    start_idx = -1\n",
    "    for i in range(id_seq.size(0) - sub_seq_len + 1):\n",
    "        if torch.equal(id_seq[i:i + sub_seq_len], img_indicator):\n",
    "            start_idx = i\n",
    "            break\n",
    "    img_file=None\n",
    "    if start_idx != -1:\n",
    "        output_ids=output_ids[:,1:start_idx+3]\n",
    "        input_ids=torch.cat((input_ids, output_ids), dim=1)\n",
    "        img_id=generate_image_vq(input_ids,model,model.get_model().vision_tower_gen.num_patches)\n",
    "        with torch.no_grad():\n",
    "            img=model.get_model().vision_tower_gen.vision_tower.decode_code(img_id,[1,8,16,16])\n",
    "        img = F.interpolate(img, size=[infer_args.image_gen_size[1], infer_args.image_gen_size[2]], mode='bicubic').permute(0, 2, 3, 1)[0]\n",
    "        img = torch.clamp(127.5 * img + 128.0, 0, 255).to(\"cpu\", dtype=torch.uint8)\n",
    "        img_file=os.path.join(infer_args.answer_image_file, f'{count}.pt')\n",
    "        torch.save(img, img_file)\n",
    "\n",
    "    ans_file.write(json.dumps({\"prompt\": cur_prompt,\n",
    "                                \"groun_truth\": groun_truth,\n",
    "                                \"answer\": outputs,\n",
    "                                \"groun_truth_img_tensor\": groun_truth_img_tensor,\n",
    "                                \"output_img_file\": img_file,\n",
    "                                \"model_id\": model_name,\n",
    "                                \"metadata\": {}}) + \"\\n\")\n",
    "    #outputs = tokenizer.batch_decode(input_ids, skip_special_tokens=False)[0].strip()\n",
    "\n",
    "print(ans_file)\n",
    "ans_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1568/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
